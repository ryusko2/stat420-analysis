---
title: "STAT-420 Data Analysis Project"
output: 
  html_document:
    toc: yes
    toc_depth: 2
    theme: simplex
---

###*Oh! The agony of an airline statistician*
####8/4/2017

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 1, digits = 4, width = 80)
```

## Introduction

[This linked repository](https://github.com/ryusko2/stat420-analysis) contains our working files for this data analysis project.

Contributing team members include:

* dakline2@illinois.edu ~ Doug Kline
* anushav3@illinois.edu ~ Anusha Varadharajan
* ryusko2@illinois.edu  ~ Ryan Yusko

We have selected a dataset representing all US domestic flights from 1990 to 2009:

[http://academictorrents.com/details/a2ccf94bbb4af222bf8e69dad60a68a29f310d9a](http://academictorrents.com/details/a2ccf94bbb4af222bf8e69dad60a68a29f310d9a)

A summary of the raw data is presented here:
  
| Short Name             | Type    | Description                                                                                              |
|------------------------|---------|----------------------------------------------------------------------------------------------------------|
| Origin                 | String  | Three letter airport code of the origin airport                                                          |
| Destination            | String  | Three letter airport code of the destination airport                                                     |
| Origin City            | String  | Origin city name                                                                                         |
| Destination City       | String  | Destination city name                                                                                    |
| Passengers             | Integer | Number of passengers transported from origin to destination                                              |
| Seats                  | Integer | Number of seats available on all flights from origin to destination                                  |
| Flights                | Integer | Number of flights between origin and destination (multiple records for one month, many with flights > 1) |
| Distance               | Integer | Distance (to nearest mile) flown between origin and destination                                          |
| Fly Date               | Integer | The date (yyyymm) of flight                                                                              |
| Origin Population      | Integer | Origin city's population as reported by US Census                                                        |
| Destination Population | Integer | Destination city's population as reported by US Census                                                   |

We hope to explore interactions and possibly model predictions for the response variable(s) `Passengers`, `Seats`, and/or `Flights`.  We have already observed high correlation between `Flights` and both `Seats` and `Passengers`, so we may not need all three of these variables in our model.  This correlation is expected because, as more flights are flown between destinations, there are more seats available, and only a certain number of passengers may fly in those seats.

We have also considered confirming busy travel months by analyzing the response to `date`.

The raw data was cleaned, and several potential predictor variables have been added:

| Predictor     | Coded as                    | Explanation                                             |
|---------------|-----------------------------|---------------------------------------------------------|
| `origin_regn` | *See region codes, below    | Region where flight originated                          |
| `origin_subr` | *See subregion codes, below | Subregion where flight originated                       |
| `dest_regn`   | *See region codes, below    | Destination region                                      |
| `dest_subr`   | *See subregion codes, below | Destination subregion                                   |
| `intra_regn`  | TRUE / FALSE                | Did flight takeoff and land in same region?             |
| `intra_subr`  | TRUE / FALSE                | Did flight takeoff and land in same subregion?          |
| `month`       | 1:12                        | Integer representation of month when flight(s) occurred |
| `month.chr`   | {"Jan", "Feb", ... , "Dec"} | String representation of month when flight(s) occurred  |

Regions as defined by the [U.S. Census Bureau](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf) are coded as follows:

| Region (code)  | Subregions Included                                    |
|----------------|--------------------------------------------------------|
| Northeast (NE) | New England, Middle Atlantic                           |
| Midwest (MW)   | East North Central, West North Central                 |
| South (S)      | South Atlantic, East South Central, West South Central |
| West (W)       | Mountain, Pacific                                      |

Subregions as defined by the [U.S. Census Bureau](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf) are coded as follows:

| Subregion (code)         | States Included*                    |
|--------------------------|-------------------------------------|
| New England (NE)         | CT, ME, MA, NH, RI, VT              |
| Middle Atlantic (MA)     | NJ, NY, PA                          |
| East North Central (ENC) | IL, IN, MI, OH, WI                  |
| West North Central (WNC) | IA, KS, MN, MO, NE, ND, SD          |
| South Atlantic (SA)      | DE, DC*, FL, GA, MD, NC, SC, VA, WV |
| East South Central (ESC) | AL, KY, MS, TN                      |
| West South Central (WSC) | AR, LA, OK, TX                      |
| Mountain (M)             | AZ, CO, ID, MT, NV, NM, UT, WY      |
| Pacific (P)              | AK, CA, HI, OR, WA                  |

\*Although Washington DC is not technically a state, it is home to two airports included in this dataset: DCA (Ronald Reagan Washington National), and IAD (Washington Dulles International).  Washington DC is in the **South Atlantic** subregion of the **South** region.

A random sample of our [[cleaned] data](https://ryusko2.web.engr.illinois.edu/files/flight_edges.csv) (**right-click to download** ~494MB) is provided below:

```{r warning = FALSE, echo = FALSE, cache = TRUE}
#load data
flight_edges = readr::read_csv("flight_edges.csv", col_types = "icccccccccclliiiiiiicii")

#remove first column
flight_edges = flight_edges[,2:ncol(flight_edges)]

#set up factor variables
flight_edges$year = as.factor(flight_edges$year)
flight_edges$origin_state = as.factor(flight_edges$origin_state)
flight_edges$dest_state = as.factor(flight_edges$dest_state)
flight_edges$origin_regn = as.factor(flight_edges$origin_regn)
flight_edges$origin_subr = as.factor(flight_edges$origin_subr)
flight_edges$dest_regn = as.factor(flight_edges$dest_regn)
flight_edges$dest_subr = as.factor(flight_edges$dest_subr)
flight_edges$intra_regn = as.factor(flight_edges$intra_regn)
flight_edges$intra_subr = as.factor(flight_edges$intra_subr)
flight_edges$month.chr = as.factor(flight_edges$month.chr)

#sample some data, removing columns containing `city`, ICAOs, and the raw `date`
knitr::kable(flight_edges[runif(5, min = 1, max = nrow(flight_edges)),-c(1:3, 7, 17)])
```

##Setup

Random sampling and exploring the data revealed two groups of observations that did not make sense, from an airline's perspective, when predicting `passengers` or `seats`:

* Flights with 0 seats
* Flights with 0 passengers

###Flights with 0 seats

Flights with zero seats could represent private, military, or cargo flights.  From an airline's perspective, these flights should not be included when trying to predict `passengers` or `seats`.

Out of 3.6M observations, `r sum(flight_edges$seats == 0)` observations have 0 seats (`r round((sum(flight_edges$seats == 0) / nrow(flight_edges)) * 100, 1)`%).

```{r}
#Remove 0-seat observations 
flight_data = subset(flight_edges, flight_edges$seats > 0)
```

###Flights with 0 passengers

Further exploration of the data has also revealed `r sum(flight_data$passengers == 0)` (out of 3.2M) observations with 0 passengers.

```{r echo = FALSE}
flight_data_0p = subset(flight_data, flight_data$passengers == 0)
```

We note that only `r round(nrow(flight_data_0p) / nrow(flight_data) * 100, 2)`% (`r nrow(flight_data_0p)`) of all remaining 3.2M observations carried 0 passengers.  Of these zero-passenger flights, `r round((sum(flight_data_0p$dist < 500) / nrow(flight_data_0p)) * 100, 2)`% flew less than 500 miles.  In fact, the histogram confirms the logical [business] decision **not** to fly aircraft empty, if at all possible...and for shorter distances if absolutely necessary:

```{r echo=FALSE, fig.height=5, fig.width=7}
hist(flight_data_0p$dist, main = "0-Passenger Distance Histogram",
     xlab = "Distance (miles)",
     ylab = "Frequency",
     col = "darkorange")
```

Airlines occasionally fly empty aircraft for maintenance purposes (for servicing at a different field), or to make an availability adjustment at an airfield where they do not have enough aircraft.  Additionally, military aircraft **will** fly 0-passenger flights (mandatory scheduled routes) where airlines **will not** fly [paid service routes] with 0 passengers.  For these reasons, it makes sense to remove the 0-passenger flights from consideration.

```{r cache = TRUE}
#pp ~ positive passenger values
flight_data_pp = subset(flight_data, flight_data$passengers > 0)
```

###Initial sampling

We initially began modeling with a sample size of 10,000 observations, without any restrictions or geographical limitation.  Later in our analysis, we realized that in order to include all factors in our sampling, we had to increase our sample size to 20,000 observations, to aid in capturing every level of `origin_state` and `dest_state`.

Before analyzing, we also removed the exactly correlated variable pairs `origin_ICAO`/`origin_city`, and `dest_ICAO`/`dest_city`.  Additionally, since we broke out the date into `year`  and `month.chr` categorical variables, we removed `date` and `month` numeric variables as well.

Although we coded regions to reduce the complexity of the full dataset, we decided as well to leave `origin_state` and `dest_state` as yet another possible "region" division, and found that these large-level categorical predictors do in fact fine tune the prediction significantly.

####Training & Test Datasets

Due to having categorical variables in the dataset with a large number of levels (i.e. `origin_state`), our training dataset had to be coerced to contain all levels in order for the `predict()` function to work on all test datasets.  The statistically sound approach to this would be to determine to what proportion each level is represented in the full dataset and then do a stratified random sample of each of those levels.  However, to simplify this problem, we chose to take a large random sample and then check that all levels were in fact captured in the sample.  If not, we sampled again until we had a sample with all levels.

Additionally, the simple `sample()` function does not guarantee the training and testing datasets are unique, but overlap as a result of sampling from 3.2 million observations is minimal.

```{r sampling}
#seed
epoch = 19700101 #The Epoch! https://en.wikipedia.org/wiki/Unix_time
set.seed(epoch)

#full 3.2M observations currently stored in `flight_data_pp`
sub_obs = 20000

#all factors in flight_data_pp; arbitrarily choose origin for states/regions
states = unique(flight_data_pp$origin_state)
region = unique(flight_data_pp$origin_regn)
subr   = unique(flight_data_pp$origin_subr)
months = unique(flight_data_pp$month.chr)

#Training subset needs to have all factor levels included for predict() to work on the test set
repeat {
  #Take a sample
  flight_sub_train = flight_data_pp[sample(1:nrow(flight_data_pp), sub_obs), -c(1:3, 7, 17, 19)]
  
  #Now let's make sure we have all the levels present
  origin_state_ok = length(intersect(flight_sub_train$origin_state, states)) == length(states)
  origin_regn_ok  = length(intersect(flight_sub_train$origin_regn,  region)) == length(region)
  origin_subr_ok  = length(intersect(flight_sub_train$origin_subr,  subr))   == length(subr)
  dest_state_ok   = length(intersect(flight_sub_train$dest_state, states))   == length(states)
  dest_regn_ok    = length(intersect(flight_sub_train$dest_regn,  region))   == length(region)
  dest_subr_ok    = length(intersect(flight_sub_train$dest_subr, subr))      == length(subr)
  month.chr_ok    = length(intersect(flight_sub_train$month.chr, months))    == length(months)
  intra_regn_ok   = sum(as.numeric(flight_sub_train$intra_regn) - 1) < sub_obs # both TRUE/FALSE
  intra_subr_ok   = sum(as.numeric(flight_sub_train$intra_subr) - 1) < sub_obs # both TRUE/FALSE
  
  if(origin_state_ok && origin_regn_ok && origin_subr_ok &&
     dest_state_ok   && dest_regn_ok   && dest_subr_ok &&
     month.chr_ok    && intra_regn_ok  && intra_subr_ok) {
    break #sample is good, we have the factor levels
  }
}

#Test subset
flight_sub_test = flight_data_pp[sample(1:nrow(flight_data_pp), sub_obs), -c(1:3, 7, 17, 19)]
```

## Methods

```{r message = FALSE, echo=FALSE, warning=FALSE}
#these libraries are required for bptest() and shapiro.test, in diagnostics function
library(lmtest)
library(MASS)

#diagnostics function
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE) {
  if(plotit == TRUE) {
    par(mfrow = c(1,2))
    plot(fitted(model), resid(model),
         xlab = "Fitted",
         ylab = "Residuals",
         main = "Fitted vs. Residuals",
         col = pcol, 
         pch = 16,
         cex = 0.35)
    abline(h = 0, col = lcol, lwd = 2)
    qqnorm(resid(model), col = pcol)
    qqline(resid(model), col = lcol)
  }
  if(testit == TRUE) {
    shapiro_wilk = shapiro.test(resid(model))
    breusch_pagan = bptest(model)
    list(shapiro_wilk = shapiro_wilk, breusch_pagan = breusch_pagan)
  }
}

#Helper function for calculating RMSE
calc_rmse = function(resids) {
  sqrt(mean(resids ^ 2, na.rm = TRUE))
}
```

```{r message = FALSE, echo=FALSE, warning=FALSE}
#this function prints the summary() of an lm object, without printing the coefficients
#it is useful for printing summaries of our largest models, without a lengthy printout
#  of all the factors
print.sum2 =
  function (x, digits = max(3L, getOption("digits") - 3L), symbolic.cor = x$symbolic.cor, 
            signif.stars = getOption("show.signif.stars"), ...) 
  {
    cat("\nCall:\n", paste(deparse(x$call), sep = "\n", collapse = "\n"), 
        "\n\n", sep = "")
    resid <- x$residuals
    df <- x$df
    rdf <- df[2L]
    cat(if (!is.null(x$weights) && diff(range(x$weights))) 
      "Weighted ", "Residuals:\n", sep = "")
    if (rdf > 5L) {
      nam <- c("Min", "1Q", "Median", "3Q", "Max")
      rq <- if (length(dim(resid)) == 2L) 
        structure(apply(t(resid), 1L, quantile), dimnames = list(nam, 
                                                                 dimnames(resid)[[2L]]))
      else {
        zz <- zapsmall(quantile(resid), digits + 1L)
        structure(zz, names = nam)
      }
      print(rq, digits = digits, ...)
    }
    else if (rdf > 0L) {
      print(resid, digits = digits, ...)
    }
    else {
      cat("ALL", df[1L], "residuals are 0: no residual degrees of freedom!")
      cat("\n")
    }
    if (length(x$aliased) == 0L) {
      #cat("\nNo Coefficients\n")
    }
    else {
      if (nsingular <- df[3L] - df[1L]) {
        #cat("\nCoefficients: (", nsingular, " not defined because of singularities)\n", sep = "")
      }
      else {
         #  cat("\nCoefficients:\n")
      }
      coefs <- x$coefficients
      if (!is.null(aliased <- x$aliased) && any(aliased)) {
        cn <- names(aliased)
        coefs <- matrix(NA, length(aliased), 4, dimnames = list(cn, 
                                                                colnames(coefs)))
        coefs[!aliased, ] <- x$coefficients
      }
      #printCoefmat(coefs, digits = digits, signif.stars = signif.stars, na.print = "NA", ...)
    }
    cat("\nResidual standard error:", format(signif(x$sigma, 
                                                    digits)), "on", rdf, "degrees of freedom")
    cat("\n")
    if (nzchar(mess <- naprint(x$na.action))) 
      cat("  (", mess, ")\n", sep = "")
    if (!is.null(x$fstatistic)) {
      cat("Multiple R-squared: ", formatC(x$r.squared, digits = digits))
      cat(",\tAdjusted R-squared: ", formatC(x$adj.r.squared, 
                                             digits = digits), "\nF-statistic:", formatC(x$fstatistic[1L], 
                                                                                         digits = digits), "on", x$fstatistic[2L], "and", 
          x$fstatistic[3L], "DF,  p-value:", format.pval(pf(x$fstatistic[1L], 
                                                            x$fstatistic[2L], x$fstatistic[3L], lower.tail = FALSE), 
                                                         digits = digits))
      cat("\n")
    }
    correl <- x$correlation
    if (!is.null(correl)) {
      p <- NCOL(correl)
      if (p > 1L) {
        cat("\nCorrelation of Coefficients:\n")
        if (is.logical(symbolic.cor) && symbolic.cor) {
          print(symnum(correl, abbr.colnames = NULL))
        }
        else {
          correl <- format(round(correl, 2), nsmall = 2, 
                           digits = digits)
          correl[!lower.tri(correl)] <- ""
          print(correl[-1, -p, drop = FALSE], quote = FALSE)
        }
      }
    }
    cat("\n")
    invisible(x)
  }
```

###Correlations

Looking at the numeric predictor values, we can see that `passengers`, `seats`, and `flights` are all highly correlated, as would be expected.  The more commercial-route flights that are flown, the more seats there are available to fill with passengers.

```{r}
#Fit a simple model
flight_model_seats_flights = lm(passengers ~ seats + flights, data = flight_sub_train)
train_rmse_seats_flights = calc_rmse(resid(flight_model_seats_flights))
test_rmse_seats_flights  = calc_rmse(flight_sub_test$passengers -  
                                     predict(flight_model_seats_flights, flight_sub_test) )

#Show correlation of the numeric predictors
flight_num = flight_sub_train[, c("passengers", "seats", "flights", "dist", "origin_pop", "dest_pop")]
cor(flight_num)
```

In fact, `r summary(flight_model_seats_flights)$adj.r.squared * 100`% of `passengers` is explained by a simple model with `seats` and `flights`.

*Model validation:*

model (`passengers` as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`

###Full-on additive

Knowing that we probably couldn't get away with a 2-predictor additive model, and to see which predictors may be worth including, we first formed a complete additive model considering all predictors, then passed it to `step()` using AIC & BIC to learn about the most significant predictors.

```{r cache = TRUE}
flight_model_big = lm(passengers ~ ., data = flight_sub_train)
n = length(resid(flight_model_big))
flight_model_big_bwd_aic = step(flight_model_big, direction = "backward", trace = 0)
flight_model_big_bwd_bic = step(flight_model_big, direction = "backward", k = log(n), trace = 0)

#Evaluate these models
train_rmse_big_bwd_aic = calc_rmse(resid(flight_model_big_bwd_aic))
test_rmse_big_bwd_aic  = calc_rmse(flight_sub_test$passengers -  
                                   predict(flight_model_big_bwd_aic, flight_sub_test) )

train_rmse_big_bwd_bic = calc_rmse(resid(flight_model_big_bwd_bic))
test_rmse_big_bwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_big_bwd_bic, flight_sub_test) )

# List results
summary(flight_model_big_bwd_aic)$call
summary(flight_model_big_bwd_bic)$call
```

The actual predictors retained during the stepwise model selection are sample dependent.  We tested several different train/test samples to see how this varied.

AIC fails to eliminate the "regional" categorical variables, normally choosing two of the three variables `origin_state`, `dest_state`, and `intra_regn`, whilst also keeping `month.chr`.  

BIC eliminates all regional categorical variables, instead preferring `month.chr` as the only categorical variable.  This smaller model may be easier to work with at first, subsequently adding region variables and testing for significance.

Also of note, BIC usually keeps `origin_pop` as a predictor and AIC usually discards it. BIC also sometimes keeps `dest_pop` as a predictor and AIC usually drops it.

*Model validation:*

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`

###Taking a closer look

To get a better idea of how potential predictor variables could be reacting, we took a look at a `pairs` plot:

```{r fig.height=8, fig.width=10}
#remove non-numerical columns
pairs(flight_sub_train[, -c(1:8, 13:14)], col = "dodgerblue")
```

Obvious linear relationships exist in both `seats` and `flights`...finite amounts of seats and flights were flown domestically, and we would expect their distributions to be very similar.

Also apparent is a very clear quadratic relationship between `flights` and `dist`.  This makes sense from an airline's perspective as well...more flights fly short distances (saving money), while fewer flights fly longer distances.

We were hoping to observe a better relationship between `passengers` and either `origin_pop` or `dest_pop`, but neither shows a distinguishable pattern in any sample we viewed.

Knowing a little more about the data allowed us to form initial [large] models to pass to `step()` using either AIC and/or BIC for selection:

```{r cache = TRUE}
#initial model best guess
flight_model_best_guess = lm(passengers ~ seats * flights * month.chr * 
                          intra_regn + origin_state + dest_state + I(dist^2) *
                          I(origin_pop^2) * I(dest_pop^2), data = flight_sub_train)

#Evaluate this model
train_rmse_best_guess = calc_rmse(resid(flight_model_best_guess))
test_rmse_best_guess  = calc_rmse(flight_sub_test$passengers -  
                                    predict(flight_model_best_guess, flight_sub_test) )

# List results
summary(flight_model_best_guess)$call
```

The regression of our initial best guess at a model proves to be significant with a p-value of `r pf(summary(flight_model_best_guess)$fstatistic[1], summary(flight_model_best_guess)$fstatistic[2], summary(flight_model_best_guess)$fstatistic[3], lower.tail = FALSE)`.  Both `seats` and `flights` were expected to be significant, as well as `dist^2`, as observed in the pairs plot.  Also significant was `intra_regn` and the vast majority of its two-way interactions.  `origin_pop` and `dest_pop` were not significant on their own; however, the interaction between `dist^2` and `origin_pop` **was** significant.  The interaction between `dist^2` and `dest_pop` was **not** significant.  This also might explain why BIC throws out the interaction with `dest_pop`.

*Model validation:*

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`

Although our educated best guess produced a model that fit the training data sample better, it did poorer on the test data sample. Running `step()` with AIC and BIC on this `fight_model_best_guess` did not produce a better model.

### Quadradic Model

Our best model so far has been the `flight_model_big_bwd_aic` which started with all predictors in an additive model and then used a backwards search using AIC for selecting $\beta$ parameters. We will start back with that model, and add an interaction between the highly correlated `seats` and `flights` and use a quadratic relationship with all numeric predictors and then allow the `step()` function to choose which are significant.

```{r cache = TRUE}
#Quadradic numeric predictors and an interaction between seats and flights
flight_model_quad = lm(passengers ~ . + seats * flights + I(seats^2) + I(flights^2) + I(dist^2) + I(origin_pop^2) + I(dest_pop^2), data = flight_sub_train)

n = length(resid(flight_model_quad))

#step selection
flight_model_quad_bwd_aic = step(flight_model_quad, direction = "backward", trace = 0)
flight_model_quad_bwd_bic = step(flight_model_quad, direction = "backward", k = log(n), trace = 0)

#Evaluate these models.
train_rmse_quad_bwd_aic = calc_rmse(resid(flight_model_quad_bwd_aic))
test_rmse_quad_bwd_aic  = calc_rmse(flight_sub_test$passengers -
                                    predict(flight_model_quad_bwd_aic, flight_sub_test))
train_rmse_quad_bwd_bic = calc_rmse(resid(flight_model_quad_bwd_bic))
test_rmse_quad_bwd_bic  = calc_rmse(flight_sub_test$passengers -
                                    predict(flight_model_quad_bwd_bic, flight_sub_test))

#List results
summary(flight_model_quad_bwd_aic)$call
summary(flight_model_quad_bwd_bic)$call

diagnostics(flight_model_quad_bwd_aic, testit = FALSE)
```

This model (`flight_model_quad_bwd_aic`) is statistically significant with a p-value of (`r pf(summary(flight_model_quad_bwd_aic)$fstatistic[1], summary(flight_model_quad_bwd_aic)$fstatistic[2], summary(flight_model_quad_bwd_aic)$fstatistic[3], lower.tail = FALSE)`). The model does appear to violate assumptions of normality and homoscedasticity, but also consistently makes good predictions across all the samples we tested.

*Model validation:*

model (`passengers` as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`
flight_model_quad_bwd_aic | `r summary(flight_model_quad_bwd_aic)$adj.r.squared` | `r train_rmse_quad_bwd_aic` | `r test_rmse_quad_bwd_aic`
flight_model_quad_bwd_bic | `r summary(flight_model_quad_bwd_bic)$adj.r.squared` | `r train_rmse_quad_bwd_bic` | `r test_rmse_quad_bwd_bic`

### Forward Search

Seeking further validation that we had found the best model, we also attempted to do a stepwise forward search.  Although we offered `R` the same formula used in `flight_model_quad`, the forward stepwise search did not pick up `origin_state` and `dest_state` and therefore did not make predictions as accurately.  A bidirectional stepwise search also arrives at the same model as a forward search.

```{r cache = TRUE}
#Start with no predictors
flight_model_fwd = lm(passengers ~ 1, data = flight_sub_train)

n = nrow(flight_sub_train)

#step selection
flight_model_quad_fwd_aic = step(flight_model_fwd, 
                                scope = passengers ~ . + seats * flights + I(seats^2) + 
                                                     I(flights^2) + I(dist^2) + 
                                                     I(origin_pop^2) + I(dest_pop^2),
                                direction = "forward", trace = 0)

flight_model_quad_fwd_bic = step(flight_model_fwd,
                                scope = passengers ~ . + seats * flights + I(seats^2) + 
                                                     I(flights^2) + I(dist^2) + 
                                                     I(origin_pop^2) + I(dest_pop^2),
                                direction = "forward", k = log(n), trace = 0)

#Evaluate these models.
train_rmse_quad_fwd_aic = calc_rmse(resid(flight_model_quad_fwd_aic))
test_rmse_quad_fwd_aic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_quad_fwd_aic, flight_sub_test))
train_rmse_quad_fwd_bic = calc_rmse(resid(flight_model_quad_fwd_bic))
test_rmse_quad_fwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_quad_fwd_bic, flight_sub_test))

#List results
summary(flight_model_quad_fwd_aic)$call
summary(flight_model_quad_fwd_bic)$call
```

###Statistical Significance

As a final validation, we notice that the simple `flight_model_seats_flights` is nested in `flight_model_quad_bwd_aic`, so we can use an ANOVA F-test to test for significance.

```{r}
(t1 = anova(flight_model_seats_flights, flight_model_quad_bwd_aic))
```

This test confirms that our selected `flight_model_quad_bwd_aic` is statistically significant with a p-value of `r t1$"Pr(>F)"[2]`, making this the best model we could find to predict `passengers`.

## Results

Below is a summary of how well the models explored thus far perform on test datasets. Many more models were explored, including variance stabilizing transforms via Box-Cox, as well as other combinations of interactions and polynomial terms.  However, these transformations and combinations did not lead to more accurate mean predictions.

model (`passengers` as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`
**flight_model_quad_bwd_aic** | `r summary(flight_model_quad_bwd_aic)$adj.r.squared` | `r train_rmse_quad_bwd_aic` | **`r test_rmse_quad_bwd_aic`**
flight_model_quad_bwd_bic | `r summary(flight_model_quad_bwd_bic)$adj.r.squared` | `r train_rmse_quad_bwd_bic` | `r test_rmse_quad_bwd_bic`
flight_model_quad_fwd_aic | `r summary(flight_model_quad_fwd_aic)$adj.r.squared` | `r train_rmse_quad_fwd_aic` | `r test_rmse_quad_fwd_aic`
flight_model_quad_fwd_bic | `r summary(flight_model_quad_fwd_bic)$adj.r.squared` | `r train_rmse_quad_fwd_bic` | `r test_rmse_quad_fwd_bic`

Our chosen model is `flight_model_quad_bwd_aic` based on having a good adjusted $R^2$ and consistently having the best root mean squared error (RMSE) on test datasets. It is a complex model summarized here:

$$
Y = \beta_0 + \beta_{1s} x_1 + \beta_{2s} x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_{7y} x_7 + \beta_{8m} x_8 + \beta_9 {x_4}^2 + \beta_{10} {x_5}^2 + \beta_{11} {x_6}^2 + \beta_{12} {x_9}^2 + \beta_{13} x_4 x_5
$$

Where:

* $x_1$ = `origin_state` = \{ AL, AL, AR, AZ, CA, CO, CT, DC, DE, FL, GA, HI, IA, ID,
                              IL, IN, KS, KY, LA, MA, MD, ME, MI, MN, MO, MS, MT, NC,
                              ND, NE, NH, NJ, NM, NV, NY, OH, OK, OR, PA, RI, SC, SD,
                              TN, TX, UT, VA, VT, WA, WI, WV, WY \}
* $x_2$ = `dest_state` = \{ AL, AL, AR, AZ, CA, CO, CT, DC, DE, FL, GA, HI, IA, ID,
                              IL, IN, KS, KY, LA, MA, MD, ME, MI, MN, MO, MS, MT, NC,
                              ND, NE, NH, NJ, NM, NV, NY, OH, OK, OR, PA, RI, SC, SD,
                              TN, TX, UT, VA, VT, WA, WI, WV, WY \}
* $x_3$ = `intra_subr` = $\begin{cases}
                            \text{TRUE,} & \text{if flight is within subregion}\\
                            \text{FALSE,} & \text{if flight destination is in different subregion}\\
                          \end{cases}$
* $x_4$ = `seats` (numeric)
* $x_5$ = `flights` (numeric)
* $x_6$ = `dist` (numeric)
* $x_7$ = `year` (factor)
* $x_8$ = `month.chr` (factor)
* $x_9$ = `dest_pop` (numeric)
* $\beta_{1s}$ = 51 coefficients corresponding to `origin_state` levels
* $\beta_{2s}$ = 51 coefficients corresponding to `dest_state` levels
* $\beta_{7y}$ = 20 coefficients corresponding to `year` levels
* $\beta_{8m}$ = 12 coefficients corresponding to `month.chr` levels

```{r}
print.sum2(summary(flight_model_quad_bwd_aic))
```

Although our chosen model (with 140 $\beta$ parameters) makes the best predictions, it does appear to violate assumptions of normality and homoscedasticity.

```{r}
diagnostics(flight_model_quad_bwd_aic, testit = FALSE)
```

## Discussion

This dataset representing all US domestic flights from 1990 to 2009 has over 3 million records, which is largely concerned with moving `passengers`.  We explored the data in multiple ways with our primary focus on predicting `passengers`.  What airline statistician (or business executive for that matter) wouldn't like to know how many passengers were going to fly in a given month?  Although the data contains several numerical and categorical predictors, what ended up having the largest influence on predicting `passengers` are the `seats` available and the number of `flights`.  This is a logical conclusion to reach, realizing that airlines seek to fill all `seats`.  Having pricing information would have made exploring this space much more interesting.

The smallest model that **explains** `passengers` in its simplest form is estimated from this sample as:

$$
passengers =  `r coef(flight_model_seats_flights)[1]` + (`r coef(flight_model_seats_flights)[2]`) \cdot seats + (`r coef(flight_model_seats_flights)[3]`) \cdot flights
$$

The regression shows that the more `seats` that are available between two points, the more `passengers` there are; however, more `flights` between two points *reduces* the number of `passengers`.  Presumably, having more `flights` provides for a larger opportunity to have empty seats.  Using the adjusted $R^2$ value, this simple model explains about `r summary(flight_model_seats_flights)$adj.r.squared` of `passengers`. 

The model we found that best predicts `passengers` is a complex model (`flight_model_quad_bwd_aic`) that includes 51 levels for `origin_state`, 51 levels for `dest_state`, 20 levels for `year` and 12 levels for `month.chr`, as well as a quadratic fit on four numeric predictors.  Having all of these beta parameters makes this much harder to explain, but it does consistently make the best mean predictions.  We tried several different training/test set samples and `flight_model_quad_bwd_aic` consistently provided the best mean prediction as measured by its root mean squared error (RMSE).  However, the gain in mean prediction accuracy doesn't appear to be very large as shown in the following table comparing the simple model to the complex model, due to `seats` and `flights` already explaining most of `passengers`.

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_quad_bwd_aic | `r summary(flight_model_quad_bwd_aic)$adj.r.squared` | `r train_rmse_quad_bwd_aic` | `r test_rmse_quad_bwd_aic`

We arrived at `flight_model_quad_bwd_aic` using a backwards stepwise search starting with all predictors, an interaction between `seats` and `flights`, and quadratic terms for all numeric variables.  AIC was used to select predictors.  For this dataset, the backwards search out-performed both a forward search and a bidirectional stepwise search.  Searching with BIC did not yield models that tested quite as well, since BIC tended to discard the large factor variables `origin_state` and `dest_state`.  However, the performance of the model chosen by BIC is not much worse than the model chosen by AIC, so an argument could be made that the less complex model is better, depending on goals or allowed flexibility.

What is interesting about the AIC backwards search is that it kept most of the numeric predictors but discarded `origin_pop`.  AIC did find that `dest_pop` was significant.  AIC also discarded the regional predictors we added with the intention of reducing complexity within the model.  We never attempted to fit a model with `origin_cty` or `dest_cty` (origin/destination city).  What we learned while testing models is that the training dataset must have all factor levels present in order to make predictions on unseen test data.  In the case of individual cities (`r length(unique(flight_data_pp$origin_cty))` origin cities, `r length(unique(flight_data_pp$dest_cty))` destination cities), that training dataset would certainly be a hard one to compose.  AIC search also found that the flight date (`month.chr` and `year`) was significant for making mean predictions.

###Prediction case study, NY-FL (Inter-region), "January 2010"

```{r echo = FALSE}
nyfl = subset(flight_data_pp, flight_data_pp$origin_state == "NY" & flight_data_pp$dest_state == "FL")
```

This mini study represents a mean prediction for the number of passengers flying on an inter-region flight from NY to FL in January of 2010.  Since our model is restricted to the factors available in `year`, we will have to use the most recent data available (January 2009) to make our mean prediction for January of the following year, adjusting other variables as necessary.

Although the mean `seats` on all flights between NY and FL in 3.2M observations is `r mean(nyfl$seats)`, there are a high number of seats typically flown between New York City airports (JFK & LGA) and Fort Lauderdale, FL.  As an airline statistician, we are curious how many passengers might want to fly in January, and we are home alone just before Christmas so this is a good time to crunch out some `R`.  

For the first mean prediction, we analyze this popular route with the following data frame, assuming we would make 50000 seats available by flying 308 flights to Fort Lauderdale, which has a new population of 5.5M:

* `seats` = 50000
* `flights` = 308
* `dist` = 1100
* `year` = "2009"
* `month.chr` = "Jan"
* `dest_pop` = 5500000

```{r}
nyfl_50k_308 = predict(flight_model_quad_bwd_aic, newdata = data.frame(origin_state = "NY", dest_state = "FL", intra_subr = "FALSE", seats = 50000, flights = 308, dist = 1100, year = "2009", month.chr = "Jan", dest_pop = 5500000))
nyfl_50k_308
```

For a seat density (seats/flights) of `r 50000/308`, our model predicts that we will have `r nyfl_50k_308[1]` passengers.  This represents a seat filled ratio of `r nyfl_50k_308[1]/50000*100`%.

The average seat density across all 3.2M observations is `r mean(flight_data_pp$seats)/mean(flight_data_pp$flights)`, so this density makes sense for a popular route.  For reference, a typical Boeing 737-800 can be configured with 175 seats, which would be a common aircraft to use on this route.

Let's just say management wasn't happy with a seat filled ratio of `r nyfl_50k_308[1]/50000*100`%.  A typical response might be to increase the seat density.  In our model, for the same number of seats, we reduce the number of flights.  Although this reduction represents a potential loss of money for the airlines, it can be accomplished operationally by overbooking. 

For instance, if we change the number of flights to 230, our seat density becomes `r 50000/230`.  This number is more than a typical 737-800 will carry (overbooking); however, we can then expect:

```{r}
nyfl_50k_230 = predict(flight_model_quad_bwd_aic, newdata = data.frame(origin_state = "NY", dest_state = "FL", intra_subr = "FALSE", seats = 50000, flights = 230, dist = 1100, year = "2009", month.chr = "Jan", dest_pop = 5500000))
nyfl_50k_230
```

Our new seat filled ratio is `r nyfl_50k_230[1]/50000*100`%, provided by "overbooking."  This also affirms our correlation between `flights` and `passengers`, in that if there are fewer `flights`, `passengers` tends to increase.

With these types of ratios, it is not hard to understand why airlines overbook their flights to maximize profits.  

###Prediction case study, IL-MN (Intra-subregion), "July 2007"

```{r echo = FALSE}
ilmn = subset(flight_data_pp, flight_data_pp$origin_state == "IL" & flight_data_pp$dest_state == "MN")
ilmn_ord_msp = subset(ilmn, ilmn$origin_ICAO == "ORD" & ilmn$dest_ICAO == "MSP" & year == "2007" & month.chr == "Jul")
```

This mini study represents a mean prediction for the number of passengers flying on an inter-region flight from IL to MO in July of 2007.

Management has informed the overworked statistician that the seat filled ratio between O'Hare and Minneapolis was `r mean(ilmn_ord_msp$passengers)/mean(ilmn_ord_msp$seats)*100`% in July of 2007, and they are wondering what their excellent leadership did to cause such an abnormally high ratio.

We did not include individual airports in our model due to the exponential effect `r length(unique(flight_data_pp$origin_ICAO))` origin airports and `r length(unique(flight_data_pp$dest_ICAO))` destination airports would have on the number of parameters added to the model.  We can, however, predict how many passengers normally would be on a flight from IL to MN, with appropriate data:

* `seats` = 25000
* `flights` = 225
* `dist` = 334
* `year` = "2007"
* `month.chr` = "Jul"
* `dest_pop` = 3200000

```{r}
ilmn_25k_225 = predict(flight_model_quad_bwd_aic, newdata = data.frame(origin_state = "IL", dest_state = "MN", intra_subr = "TRUE", seats = 25000, flights = 225, dist = 334, year = "2007", month.chr = "Jul", dest_pop = 3200000))
ilmn_25k_225
```

A seat density of `r 25000/225` is typical for the short flight between IL and MN; the route would typically be sourced by a 100-seat [Embraer E190](https://en.wikipedia.org/wiki/Embraer_E-Jet_family#E-190_and_195).

Our model predicts that on average, flights from IL to MN would contain `r ilmn_25k_225[1]` passengers.  This mean prediction corresponds to a seat filled ratio of `r ilmn_25k_225[1]/25000*100`%.

Being the underpaid statistician you are, however, you quickly inform management that even though we only expected `r ilmn_25k_225[1]/25000*100`% of our seats to be filled, `r mean(ilmn_ord_msp$passengers)/mean(ilmn_ord_msp$seats)*100`% were likely filled due to U of I students flocking to [Mall of America](https://en.wikipedia.org/wiki/Mall_of_America) to buy up an overstock of newly-released [Apple first-generation iPhones](https://en.wikipedia.org/wiki/IPhone_(1st_generation)), and therefore *not* due to their excellent leadership decisions.

As can be seen in our two mini studies of inter- and intra-region travel, our chosen model is useful for a number of prediction applications.  Not only can it *accurately* predict mean passengers based on a number of variables, but it can also be used in a number of operational applications (cost analysis of changing seat densities, overbooking, etc.).

It bears repeating that inclusion of cost data would make this analysis more interesting...one has to think there is a direct correlation between passengers and ticket price.  However, ticket prices are subject to high variability, and could over-complicate the model.

About halfway through our analysis, we discovered that the size of our dataset and the results we were finding lent themselves to a prediction model.  This is also the status quo of an airline statistician trying to maximize filled seats on their aircraft.  Since the goal of our model became prediction, we focused upon one thing and one thing only: RMSE.  Focusing on a prediction model allowed us to table the problems we had first experienced while trying to meet normality and homoscedasticity assumptions, and dealing with high collinearity between multiple variables.  This approach also led to a highly accurate and useful model for making `passenger` mean predictions in a wide variety of applications.  

##Appendix

###Passengers vs. Month (High traffic significance?)

*This quick mini-analysis was more of a curiosity.  Since it was not really used in determining the final model selected, we include it here, as an appendix.*

For starters, we were curious to see whether `passengers` could be modeled as a response of `month` as a factor variable, verifying high-traffic seasons of flying.

```{r}
month_model = lm(passengers ~ month.chr, data = flight_sub_train)
summary(month_model, data = flight_sub_train)
```

Even though the p-value of this regression (`r pf(summary(month_model)$fstatistic[1], summary(month_model)$fstatistic[2], summary(month_model)$fstatistic[3], lower.tail = FALSE)`) is significant, the simple representation did not prove very fruitful at all, with an adjusted $R^2$ of `r summary(month_model)$adj.r.squared`.

Some samplings seemed to verify heavy travel months in November & December; however, other samplings placed heavier travel months in February & March.  In the end, the high traffic significance did not prove any one month was heaver in air travel than any other, on average.  People seem to travel around the country at all times, with business travel (surmisedly) masking out any significance in originally-anticipated high-travel months.

###Stepwise "big" model diagnostics

*When we initially started analyzing the big model selected by AIC & BIC, we ran into problems convincing ourselves that these models met normality and homoscedasticity assumptions.  Since our analysis focused upon prediction, these attempts were eventually abandoned, but are included here for your amusement.*

```{r fig.height=5, fig.width=10, echo = FALSE}
diagnostics(flight_model_best_guess, testit = FALSE)
```

While it is nice to have so many significant predictors, we appear to have an issue with both homoscedasticity and normality, confirmed by fitted vs. residuals and Q-Q plots.

We wished to perform a response variable transformation to address the clear pattern in the fitted vs. residuals plot.  To determine an appropriate transformation, we ran a Box-Cox log-likelihood plot:

```{r fig.width = 7, fig.height = 5}
boxcox(flight_model_best_guess, lambda = seq(0.55, 0.60, 1/100))
```

We chose a Box-Cox transformation with $\lambda = 0.575$.  

```{r}
lambda = 0.575

#Transform the response to improve on normality of the residual errors
transform = function(y) {
  ((y ^ lambda) - 1) / lambda
}

reverse_transform = function(y) {
  (y * lambda + 1) ^ (1 / lambda)
}

min_pass = min(flight_sub_train$passengers)
max_pass = max(flight_sub_train$passengers)

min_trans_pass = min(transform(flight_sub_train$passengers))
max_trans_pass = max(transform(flight_sub_train$passengers))
```

In the sampled dataset, the range of passengers is `r min_pass` to `r max_pass`. The Box-Cox transforms this into a much tighter range of `r min_trans_pass` to `r max_trans_pass`.

```{r cache = TRUE}
#perform response transformation
flight_model_box = lm((passengers ^ lambda - 1) / lambda ~ 
                       seats * flights * month.chr * intra_regn + 
                       origin_state + dest_state + I(dist^2) * I(origin_pop^2) * I(dest_pop^2), 
                       data = flight_sub_train)
n = length(resid(flight_model_box))

#step selection
flight_model_box_bwd_aic = step(flight_model_box, direction = "backward", trace = 0)
flight_model_box_bwd_bic = step(flight_model_box, direction = "backward", k = log(n), trace = 0)

#Evaluate these models. Since the predictions are in the Box-Cox transformed units,
#we need to transform back for comparison with other models.
train_rmse_box = calc_rmse(flight_sub_train$passengers -
                           reverse_transform(predict(flight_model_box)))
test_rmse_box  = calc_rmse(flight_sub_test$passengers -
                           reverse_transform(predict(flight_model_box, flight_sub_test)))
train_rmse_box_bwd_aic = calc_rmse(flight_sub_train$passengers -
                                   reverse_transform(predict(flight_model_box_bwd_aic)))
test_rmse_box_bwd_aic  = calc_rmse(flight_sub_test$passengers -
                               reverse_transform(predict(flight_model_box_bwd_aic, flight_sub_test)))
train_rmse_box_bwd_bic = calc_rmse(flight_sub_train$passengers -
                                   reverse_transform(predict(flight_model_box_bwd_bic)))
test_rmse_box_bwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   reverse_transform(predict(flight_model_box_bwd_bic, flight_sub_test)))

#List results
summary(flight_model_box_bwd_aic)$call
summary(flight_model_box_bwd_bic)$call

diagnostics(flight_model_box_bwd_aic, testit = FALSE)
```

While the Fitted vs. Residuals and Q-Q plots have improved dramatically, they are not quite ideal; however, they are probably as good as they are going to get, given the Box-Cox transformation we have chosen.

AIC still likes holding on to `origin_state` and `dest_state`, while BIC seems happy to remove them from the model. Both AIC & BIC retain some sort of region coding, however, as BIC keeps `intra_regn` in its model.

*Model validation:*

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`
flight_model_quad_bwd_aic | `r summary(flight_model_quad_bwd_aic)$adj.r.squared` | `r train_rmse_quad_bwd_aic` | `r test_rmse_quad_bwd_aic`
flight_model_quad_bwd_bic | `r summary(flight_model_quad_bwd_bic)$adj.r.squared` | `r train_rmse_quad_bwd_bic` | `r test_rmse_quad_bwd_bic`
flight_model_box | `r summary(flight_model_box)$adj.r.squared` | `r train_rmse_box` | `r test_rmse_box`
flight_model_box_bwd_aic | `r summary(flight_model_box_bwd_aic)$adj.r.squared` | `r train_rmse_box_bwd_aic` | `r test_rmse_box_bwd_aic`
flight_model_box_bwd_bic | `r summary(flight_model_box_bwd_bic)$adj.r.squared` | `r train_rmse_box_bwd_bic` | `r test_rmse_box_bwd_bic`

However, as shown in the above model validation table, while this Box-Cox transformed model has improved normality and equal variance assumptions, it has more errors in its mean predictions and appears to have overfit the data. We tried an additional variety of predictors, with and without interactions, with and without quadratics, but could not find a model with a Box-Cox translated response that performed well on this sample.

###Looking at influential observations

*During analysis when we were "tightening" up our model, we also took a look at influential observations, that usually fell into one of several categories:*

* Short high volume flights
* Medium- to long-distance low volume flights

```{r, warning=FALSE}
n = length(resid(flight_model_quad_bwd_aic))

percent = sum(cooks.distance(flight_model_quad_bwd_aic) >
              (4 / length(cooks.distance(flight_model_quad_bwd_aic))), na.rm = TRUE) / n * 100

plot(flight_model_quad_bwd_aic, which = 6)
```

About `r percent`% of the training dataset are influential observations according to Cook's Distance heuristic.  Some of these observations are very large. However, these values appear to be valid data.  An example of a high influence observation comes from high volume short hops between the Hawaiian Islands. Removing some of the top influential observations does improved the residual errors slightly, but we cannot justify doing so.

###Seats as the response variable

*After analyzing the initial `passenger` model, we were curious how well we could model `seats` as a response variable. Since `passengers` and `seats` are highly correlated, this analysis came out very similar to models for `passengers`.*

We have followed a similar procedure to find the ideal model with seats as a response variable.  We then checked whether the model adhered to the assumptions.  However, we found that the model was similar to the big model in terms of the fitted values and we attribute this to the fact that passengers and seats are highly correlated. Therefore, we continued with using the big model. 

```{r}
# Simple model for comparison
flight_model_pass_flights = lm(seats ~ passengers + flights, data = flight_sub_train)

train_rmse_pass_flights = calc_rmse(resid(flight_model_pass_flights))

test_rmse_pass_flights  = calc_rmse(flight_sub_test$seats -
                                    predict(flight_model_pass_flights, flight_sub_test))

```

Below, we are starting off with a model that uses seats as an alternative response and then let AIC or BIC choose the best model. The model that we have started off with has an interaction between `passengers` and `flights` because these predictors are highly correlated. We have also chosen polynomial terms of `passengers` and `flights` because even though the correlation scatter plot shows that it is linear, we were curious to see whether the `step()` function using AIC or BIC criteria would choose these terms. We chose the polynomial terms of `dist`, `origin_pop`, and `dest_pop` due to the way the graph looks in the correlation scatter plot.  

```{r cache = TRUE}
flight_model_seats = lm(seats ~ . + 
                                passengers * flights + I(passengers^2) + 
                                I(flights^2) + I(dist^2) + 
                                I(origin_pop^2) + I(dest_pop^2), 
                                data = flight_sub_train)

n = length(resid(flight_model_seats))

flight_model_seats_bwd_aic = step(flight_model_seats, direction = "backward", trace = 0)
flight_model_seats_bwd_bic = step(flight_model_seats, direction = "backward", 
                                  k = log(n), trace = 0)

#Evaluate these models.

train_rmse_seats_bwd_aic = calc_rmse(resid(flight_model_seats_bwd_aic))
test_rmse_seats_bwd_aic  = calc_rmse(flight_sub_test$seats -
                                   predict(flight_model_seats_bwd_aic, flight_sub_test))
train_rmse_seats_bwd_bic = calc_rmse(resid(flight_model_seats_bwd_bic))
test_rmse_seats_bwd_bic  = calc_rmse(flight_sub_test$seats -
                                   predict(flight_model_seats_bwd_bic, flight_sub_test))

#List results
summary(flight_model_seats_bwd_aic)$call
summary(flight_model_seats_bwd_bic)$call
```

*Model validation:*

model (`seats` as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_pass_flights | `r summary(flight_model_pass_flights)$adj.r.squared` | `r train_rmse_pass_flights` | `r test_rmse_pass_flights`
flight_model_seats_bwd_aic | `r summary(flight_model_seats_bwd_aic)$adj.r.squared` | `r train_rmse_seats_bwd_aic` | `r test_rmse_seats_bwd_aic`
flight_model_seats_bwd_bic | `r summary(flight_model_seats_bwd_bic)$adj.r.squared` | `r train_rmse_seats_bwd_bic` | `r test_rmse_seats_bwd_bic`

Due to how correlated `seats`, `passengers` and `flights` are, the mean prediction power of a linear regression model is about the same for `seats` as it is for `passengers`. We further checked assumptions of normality and homoscedasticity and found similar violations to the models for `passengers`. Attempting a Box-Cox variance stabilizing transform on the `seats` response produced a model with higher RMSE values than the results shown above.

###Flights as the response variable

*After analyzing the initial `passenger` model, we were curious how well we could model `flights` as a response variable.*

Again, we wanted to verify that the model that used flights as a response variable would meet the normality and homoscedasticity assumptions. After generating the model using `step()`, we noticed that this model is quite similar to the initial model because of the high correlation between flights and passengers and hence, we abandoned our efforts in this direction. 

```{r cache = TRUE}
flight_model_pass_seats = lm(flights ~ passengers + seats, data = flight_sub_train)

train_rmse_pass_seats   = calc_rmse(resid(flight_model_pass_seats))
test_rmse_pass_seats    = calc_rmse(flight_sub_test$flights -
                                    predict(flight_model_pass_seats, flight_sub_test))

flight_model_flights = lm(flights ~ . + 
                                    passengers * seats + I(passengers^2) + 
                                    I(seats^2) + I(dist^2) + 
                                    I(origin_pop^2) + I(dest_pop^2), 
                                    data = flight_sub_train)

n = length(resid(flight_model_flights))

flight_model_flights_bwd_aic = step(flight_model_flights, 
                                  direction = "backward", trace = 0)
flight_model_flights_bwd_bic = step(flight_model_flights, 
                                  direction = "backward", k = log(n), trace = 0)

#Evaluate these models.
train_rmse_flights_bwd_aic = calc_rmse(resid(flight_model_flights_bwd_aic))
test_rmse_flights_bwd_aic  = calc_rmse(flight_sub_test$flights -
                                       predict(flight_model_flights_bwd_aic,
                                               flight_sub_test))
train_rmse_flights_bwd_bic = calc_rmse(resid(flight_model_flights_bwd_bic))
test_rmse_flights_bwd_bic  = calc_rmse(flight_sub_test$flights -
                                       predict(flight_model_flights_bwd_bic,
                                               flight_sub_test))

#List results
summary(flight_model_flights_bwd_aic)$call
summary(flight_model_flights_bwd_bic)$call
```

*Model validation:*

model (`flights` as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_flights | `r summary(flight_model_flights)$adj.r.squared` | `r train_rmse_pass_seats` | `r test_rmse_pass_seats`
flight_model_flights_bwd_aic | `r summary(flight_model_flights_bwd_aic)$adj.r.squared` | `r train_rmse_flights_bwd_aic` | `r test_rmse_flights_bwd_aic`
flight_model_flights_bwd_bic | `r summary(flight_model_flights_bwd_bic)$adj.r.squared` | `r train_rmse_flights_bwd_bic` | `r test_rmse_flights_bwd_bic`

Although `passengers` and `flights` are highly correlated, the mean predictive power for this model is lower than for `passengers`, with an adjusted $R^2$ value of about `r summary(flight_model_flights_bwd_aic)$adj.r.squared`. Airplanes come in multiple sizes, so airlines have much more flexibility in selecting which size airplanes to fly on a particular route, which will result in more or less flights for a passenger load. Also, airlines may desire to have multiple smaller flights with more departure/arrival times, or less departure/arrival times using larger planes depending on connection needs.

Since there are a much smaller number of `flights` per route than `passengers`, this results in a much smaller root mean squared error (RMSE) value which is not comparable to the RMSE values for `passengers` models.

###Empty seats as the response variable

*It is also possible to take a slightly different look at the data and model empty seats for each record.*

```{r cache = TRUE}
# Make a temporary copies
flight_sub_train2 = flight_sub_train
flight_sub_test2  = flight_sub_test

# Add an empty seat column
flight_sub_train2$empty = flight_sub_train2$seats - flight_sub_train2$passengers
flight_sub_test2$empty  = flight_sub_test2$seats  - flight_sub_test2$passengers

fight_model_empty = lm(empty ~ . - passengers - seats + 
                      I(seats^2) + I(flights^2) + I(dist^2) + I(origin_pop^2) +
                      I(dest_pop^2), data = flight_sub_train2)

n = length(resid(fight_model_empty))

#step selection
flight_model_empty_bwd_aic = step(fight_model_empty, direction = "backward", trace = 0)
flight_model_empty_bwd_bic = step(fight_model_empty, direction = "backward", k = log(n), trace = 0)

#Evaluate these models.
train_rmse_empty_bwd_aic = calc_rmse(resid(flight_model_empty_bwd_aic))
test_rmse_empty_bwd_aic  = calc_rmse(flight_sub_test2$empty -
                                     predict(flight_model_empty_bwd_aic,
                                             flight_sub_test2))
train_rmse_empty_bwd_bic = calc_rmse(resid(flight_model_empty_bwd_bic))
test_rmse_empty_bwd_bic  = calc_rmse(flight_sub_test2$empty -
                                     predict(flight_model_empty_bwd_bic,
                                             flight_sub_test2))

#List results
summary(flight_model_empty_bwd_aic)$call
summary(flight_model_empty_bwd_bic)$call
```

model (`empty` as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_empty_bwd_aic | `r summary(flight_model_empty_bwd_aic)$adj.r.squared` | `r train_rmse_empty_bwd_aic` | `r test_rmse_empty_bwd_aic`
flight_model_empty_bwd_bic | `r summary(flight_model_empty_bwd_bic)$adj.r.squared` | `r train_rmse_empty_bwd_bic` | `r test_rmse_empty_bwd_bic`

The regression analysis is significant and the predictors do predict a large proportion of `empty`.
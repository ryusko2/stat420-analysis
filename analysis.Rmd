---
title: "STAT-420 Data Analysis Project"
output: 
  html_document:
    toc: yes
    toc_depth: 2
    theme: simplex
---

###*Oh! The agony of an airline statistician*
####8/4/2017

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 1, digits = 4, width = 80)

```


## Introduction

[This linked repository](https://github.com/ryusko2/stat420-analysis) contains our working files for this data analysis project.

Contributing team members include:

* dakline2@illinois.edu ~ Doug Kline
* anushav3@illinois.edu ~ Anusha Varadharajan
* ryusko2@illinois.edu  ~ Ryan Yusko

We have selected a dataset representing all US domestic flights from 1990 to 2009:

[http://academictorrents.com/details/a2ccf94bbb4af222bf8e69dad60a68a29f310d9a](http://academictorrents.com/details/a2ccf94bbb4af222bf8e69dad60a68a29f310d9a)

A summary of the raw data is presented here:
  
| Short Name             | Type    | Description                                                                                              |
|------------------------|---------|----------------------------------------------------------------------------------------------------------|
| Origin                 | String  | Three letter airport code of the origin airport                                                          |
| Destination            | String  | Three letter airport code of the destination airport                                                     |
| Origin City            | String  | Origin city name                                                                                         |
| Destination City       | String  | Destination city name                                                                                    |
| Passengers             | Integer | Number of passengers transported from origin to destination                                              |
| Seats                  | Integer | Number of seats available on all flights from origin to destination                                  |
| Flights                | Integer | Number of flights between origin and destination (multiple records for one month, many with flights > 1) |
| Distance               | Integer | Distance (to nearest mile) flown between origin and destination                                          |
| Fly Date               | Integer | The date (yyyymm) of flight                                                                              |
| Origin Population      | Integer | Origin city's population as reported by US Census                                                        |
| Destination Population | Integer | Destination city's population as reported by US Census                                                   |

We hope to explore interactions and possibly model predictions for the response variable(s) `Passengers`, `Seats`, and/or `Flights`.  We have already observed high correlation between `Flights` and both `Seats` and `Passengers`, so we may not need all three of these variables in our model.  This correlation is expected because, as more flights are flown between destinations, there are more seats available, and only a certain number of passengers may fly in those seats.

We have also considered confirming busy travel months by analyzing the response to `date`.

The raw data was cleaned, and several potential predictor variables have been added:

| Predictor     | Coded as                    | Explanation                                             |
|---------------|-----------------------------|---------------------------------------------------------|
| `origin_regn` | *See region codes, below    | Region where flight originated                          |
| `origin_subr` | *See subregion codes, below | Subregion where flight originated                       |
| `dest_regn`   | *See region codes, below    | Destination region                                      |
| `dest_subr`   | *See subregion codes, below | Destination subregion                                   |
| `intra_regn`  | TRUE / FALSE                | Did flight takeoff and land in same region?             |
| `intra_subr`  | TRUE / FALSE                | Did flight takeoff and land in same subregion?          |
| `month`       | 1:12                        | Integer representation of month when flight(s) occurred |
| `month.chr`   | {"Jan", "Feb", ... , "Dec"} | String representation of month when flight(s) occurred  |

Regions as defined by the [U.S. Census Bureau](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf) are coded as follows:

| Region (code)  | Subregions Included                                    |
|----------------|--------------------------------------------------------|
| Northeast (NE) | New England, Middle Atlantic                           |
| Midwest (MW)   | East North Central, West North Central                 |
| South (S)      | South Atlantic, East South Central, West South Central |
| West (W)       | Mountain, Pacific                                      |

Subregions as defined by the [U.S. Census Bureau](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf) are coded as follows:

| Subregion (code)         | States Included*                    |
|--------------------------|-------------------------------------|
| New England (NE)         | CT, ME, MA, NH, RI, VT              |
| Middle Atlantic (MA)     | NJ, NY, PA                          |
| East North Central (ENC) | IL, IN, MI, OH, WI                  |
| West North Central (WNC) | IA, KS, MN, MO, NE, ND, SD          |
| South Atlantic (SA)      | DE, DC*, FL, GA, MD, NC, SC, VA, WV |
| East South Central (ESC) | AL, KY, MS, TN                      |
| West South Central (WSC) | AR, LA, OK, TX                      |
| Mountain (M)             | AZ, CO, ID, MT, NV, NM, UT, WY      |
| Pacific (P)              | AK, CA, HI, OR, WA                  |

\*Although Washington DC is not technically a state, it is home to two airports included in this dataset: DCA (Ronald Reagan Washington National), and IAD (Washington Dulles International).  Washington DC is in the **South Atlantic** subregion of the **South** region.

A random sample of our [[cleaned] data](https://ryusko2.web.engr.illinois.edu/files/flight_edges.csv) (**right-click to download** ~494MB) is provided below:

```{r warning = FALSE, echo = FALSE, cache = TRUE}
#load data
flight_edges = readr::read_csv("flight_edges.csv", col_types = "icccccccccclliiiiiiicii")

#remove first column
flight_edges = flight_edges[,2:ncol(flight_edges)]

#set up factor variables
flight_edges$year = as.factor(flight_edges$year)
flight_edges$origin_state = as.factor(flight_edges$origin_state)
flight_edges$dest_state = as.factor(flight_edges$dest_state)
flight_edges$origin_regn = as.factor(flight_edges$origin_regn)
flight_edges$origin_subr = as.factor(flight_edges$origin_subr)
flight_edges$dest_regn = as.factor(flight_edges$dest_regn)
flight_edges$dest_subr = as.factor(flight_edges$dest_subr)
flight_edges$intra_regn = as.factor(flight_edges$intra_regn)
flight_edges$intra_subr = as.factor(flight_edges$intra_subr)
flight_edges$month.chr = as.factor(flight_edges$month.chr)

#sample some data, removing columns containing `city`, ICAOs, and the raw `date`
knitr::kable(flight_edges[runif(5, min = 1, max = nrow(flight_edges)),-c(1:3, 7, 17)])
```

##Setup

Random sampling and exploring the data has revealed two groups of observations that do not make sense, from an airline's perspective, when predicting `passengers` or `seats`:

* Flights with 0 seats
* Flights with 0 passengers

###Flights with 0 seats

Flights with zero seats could represent private or military flights.  From an airline's perspective, these flights should not be included when trying to predict `passengers` or `seats`.

Out of 3.6M observations, `r sum(flight_edges$seats == 0)` observations have 0 seats (`r round((sum(flight_edges$seats == 0) / nrow(flight_edges)) * 100, 1)`%).

```{r}
#Remove 0-seat observations 
flight_data = subset(flight_edges, flight_edges$seats > 0)
```

###Flights with 0 passengers

Further exploration of the data has also revealed `r sum(flight_data$passengers == 0)` (out of 3.2M) observations with 0 passengers.

```{r echo = FALSE}
flight_data_0p = subset(flight_data, flight_data$passengers == 0)
```

We note that only `r round(nrow(flight_data_0p) / nrow(flight_data) * 100, 2)`% (`r nrow(flight_data_0p)`) of all remaining 3.2M observations carried 0 passengers.  Of these zero-passenger flights, `r round((sum(flight_data_0p$dist < 500) / nrow(flight_data_0p)) * 100, 2)`% flew less than 500 miles.  In fact, the histogram confirms the logical [business] decision **not** to fly aircraft empty, if at all possible...and for shorter distances if absolutely necessary:

```{r echo=FALSE, fig.height=5, fig.width=7}
hist(flight_data_0p$dist, main = "0-Passenger Distance Histogram",
     xlab = "Distance (miles)",
     ylab = "Frequency",
     col = "darkorange")
```

Airlines occasionally fly empty aircraft for maintenance purposes (for servicing at a different field), or to make an availability adjustment at an airfield where they do not have enough aircraft.  Additionally, military aircraft **will** fly 0-passenger flights (mandatory scheduled routes) where airlines **will not** fly [paid service routes] with 0 passengers.  For these reasons, it makes sense to remove the 0-passenger flights from consideration.

```{r cache = TRUE}
#pp ~ positive passenger values
flight_data_pp = subset(flight_data, flight_data$passengers > 0)
```

###Initial sampling

Based on a recommendation overheard in help sessions, we initially began modeling with 10,000 observations, without any restrictions or geographical limitation.  Later in our analysis, we realized that in order to include all factors in our sampling, we increased our sample size to 20,000 observations, to aid in capturing every level of specific factor variables present in this dataset.

Before analyzing, we also removed the informational variables `origin_ICAO`, `dest_ICAO`, `origin_city`, and `dest_city`.  Additionally, since we broke out the date into `year`  and `month.chr` categorical variables, we removed `date` and `month` numeric variables as well.

Although we coded regions to reduce the complexity of the full dataset, we decided as well to leave `origin_state` and `dest_state` as yet another possible "region" division, and found that these large-level categorical predictors do in fact fine tune the prediction significantly.

####Training & Test Datasets

Due to having categorical variables in the dataset with a large number of levels (i.e. `origin_state`), our training dataset had to be coerced to contain all levels in order for the `predict()` function to work on all test datasets.  The statistically sound approach to this would be to determine to what proportion each level is represented in the full dataset and then do a stratified random sample of each of those levels.  However, to simplify this problem, we chose to take a large random sample and then check that all levels were in fact captured in the sample.  If not, we sampled again until we had a sample with all levels.

Additionally, the simple `sample()` function does not guarantee the training and testing datasets are unique, but overlap as a result of sampling from 3.2 million observations is minimal.

```{r sampling}
#seed
epoch = 19700101 #The Epoch! https://en.wikipedia.org/wiki/Unix_time
set.seed(epoch)


#full 3.2M observations currently stored in `flight_data_pp`
sub_obs = 20000

# #All 50 states and DC
# states = c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", 
#            "GA", "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA",
#            "MD", "ME", "MI", "MN", "MO", "MS", "MT", "NC", "ND", "NE",
#            "NH", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "RI",
#            "SC", "SD", "TN", "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")
# 
# region = c("NE", "MW", "S", "W")
# 
# subr   = c("NE", "MA", "ENC", "WNC", "SA", "ESC", "WSC", "M", "P")
# 
# months = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

#all factors in flight_data_pp; arbitrarily choose origin for states/regions
states = unique(flight_data_pp$origin_state)
region = unique(flight_data_pp$origin_regn)
subr   = unique(flight_data_pp$origin_subr)
months = unique(flight_data_pp$month.chr)

#Training subset needs to have all factor levels included for predict() to work on the test set
repeat {
  #Take a sample
  flight_sub_train = flight_data_pp[sample(1:nrow(flight_data_pp), sub_obs), -c(1:3, 7, 17, 19)]
  
  #Now let's make sure we have all the levels present
  origin_state_ok = length(intersect(flight_sub_train$origin_state, states)) == length(states)
  origin_regn_ok  = length(intersect(flight_sub_train$origin_regn,  region)) == length(region)
  origin_subr_ok  = length(intersect(flight_sub_train$origin_subr,  subr))   == length(subr)
  dest_state_ok   = length(intersect(flight_sub_train$dest_state, states))   == length(states)
  dest_regn_ok    = length(intersect(flight_sub_train$dest_regn,  region))   == length(region)
  dest_subr_ok    = length(intersect(flight_sub_train$dest_subr, subr))      == length(subr)
  month.chr_ok    = length(intersect(flight_sub_train$month.chr, months))    == length(months)
  intra_regn_ok   = sum(as.numeric(flight_sub_train$intra_regn) - 1) < sub_obs # both TRUE/FALSE
  intra_subr_ok   = sum(as.numeric(flight_sub_train$intra_subr) - 1) < sub_obs # both TRUE/FALSE
  
  if(origin_state_ok && origin_regn_ok && origin_subr_ok &&
     dest_state_ok   && dest_regn_ok   && dest_subr_ok &&
     month.chr_ok && intra_regn_ok && intra_subr_ok) {
    break #sample is good, we have the factor levels
  }
}

#Test subset
flight_sub_test = flight_data_pp[sample(1:nrow(flight_data_pp), sub_obs), -c(1:3, 7, 17, 19)]
```

## Methods

```{r message = FALSE, echo=FALSE, warning=FALSE}
#these libraries are required for bptest() and shapiro.test, in diagnostics function
library(lmtest)
library(MASS)

#diagnostics function
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE) {
  if(plotit == TRUE) {
    par(mfrow = c(1,2))
    plot(fitted(model), resid(model),
         xlab = "Fitted",
         ylab = "Residuals",
         main = "Fitted vs. Residuals",
         col = pcol, 
         pch = 16,
         cex = 0.35)
    abline(h = 0, col = lcol, lwd = 2)
    qqnorm(resid(model), col = pcol)
    qqline(resid(model), col = lcol)
  }
  if(testit == TRUE) {
    shapiro_wilk = shapiro.test(resid(model))
    breusch_pagan = bptest(model)
    list(shapiro_wilk = shapiro_wilk, breusch_pagan = breusch_pagan)
  }
}

#Helper function for calculating RMSE
calc_rmse = function(resids) {
  sqrt(mean(resids ^ 2, na.rm = TRUE))
}

```

###Passengers vs. Month (High traffic significance?)

For starters, we were curious to see whether `passengers` could be modeled as a response of `month` as a factor variable, verifying high-traffic seasons of flying.

```{r}
month_model = lm(passengers ~ month.chr, data = flight_sub_train)
summary(month_model, data = flight_sub_train)
```

Even though the p-value of this regression (`r pf(summary(month_model)$fstatistic[1], summary(month_model)$fstatistic[2], summary(month_model)$fstatistic[3], lower.tail = FALSE)`) is significant, the simple representation did not prove very fruitful at all, with an adjusted $R^2$ of `r summary(month_model)$adj.r.squared`.

Some samplings seemed to verify heavy travel months in November & December; however, other samplings placed heavier travel months in February & March.  In the end, the high traffic significance did not prove any one month was heaver in air travel than any other, on average.  People seem to travel around the country at all times, with business travel (surmisingly) masking out any significance in originally-anticipated high-travel months.

###Correlations

Looking at the numeric predictor values, we can see that `passengers`, `seats`, and `flights` are all highly correlated, as would be expected.  The more commercial-route flights that are flown, the more seats there are available to fill with passengers.

```{r}
#Fit a simple model
flight_model_seats_flights = lm(passengers ~ seats + flights, data = flight_sub_train)
train_rmse_seats_flights = calc_rmse(resid(flight_model_seats_flights))
test_rmse_seats_flights  = calc_rmse(flight_sub_test$passengers -  
                                     predict(flight_model_seats_flights, flight_sub_test) )

#Show correlation of the numeric predictors
flight_num = flight_sub_train[, c("passengers", "seats", "flights", "dist", "origin_pop", "dest_pop")]
cor(flight_num)
```

In fact, `r summary(flight_model_seats_flights)$adj.r.squared * 100`% of `passengers` is explained by a simple model with `seats` and `flights`.

*Model validation:*

model (passengers as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`

###Full-on additive

To see which predictors may be worth including, we first formed a simple additive model considering all predictors, then passed it to `step()` using AIC & BIC to get a first opinion.

```{r cache = TRUE}
flight_model_big = lm(passengers ~ ., data = flight_sub_train)
n = length(resid(flight_model_big))
flight_model_big_bwd_aic = step(flight_model_big, direction = "backward", trace = 0)
flight_model_big_bwd_bic = step(flight_model_big, direction = "backward", k = log(n), trace = 0)

#Evaluate these models
train_rmse_big_bwd_aic = calc_rmse(resid(flight_model_big_bwd_aic))
test_rmse_big_bwd_aic  = calc_rmse(flight_sub_test$passengers -  
                                   predict(flight_model_big_bwd_aic, flight_sub_test) )

train_rmse_big_bwd_bic = calc_rmse(resid(flight_model_big_bwd_bic))
test_rmse_big_bwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_big_bwd_bic, flight_sub_test) )

# List results
summary(flight_model_big_bwd_aic)$call
summary(flight_model_big_bwd_bic)$call

```

The actual predictors retained during the stepwise model selection is sample dependent. We tested several different train/test samples to see how this varied.

AIC fails to eliminate the "regional" categorical variables, normally choosing two of the three variables `origin_state`, `dest_state`, and `intra_regn`, whilst also keeping `month.chr`.  

BIC eliminates all regional categorical variables, instead preferring `month.chr` as the only categorical variable.  The smaller model may be easier to work with at first, subsequently adding region variables and testing for significance.

Also of note, BIC usually keeps `origin_pop` as a predictor and AIC usually discards it. BIC also sometimes keeps `dest_pop` as a predictor and AIC usually drops it.

*Model validation:*

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`

###Taking a closer look

To get a better idea of how potential predictor variables could be reacting, we took a look at a `pairs` plot:

```{r fig.height=8, fig.width=10}
#remove non-numerical columns
pairs(flight_sub_train[, -c(1:8, 13:14)], col = "dodgerblue")
```

Obvious linear relationships exist in both `seats` and `flights`...finite amounts of seats and flights were flown domestically, and we would expect their distributions to be very similar.

Also apparent is a very clear quadratic relationship between `flights` and `dist`.  This makes sense from an airline's perspective as well...more flights fly short distances (saving money), while fewer flights fly longer distances.

We were hoping to **observe** a better relationship between `passengers` and either `origin_pop` or `dest_pop`, but neither shows a distinguishable pattern in any sample we viewed.

Knowing a little more about the data allowed us to form initial [large] models to pass to `step()` using either AIC and/or BIC for selection:


```{r}
#initial model best guess
flight_model_best_guess = lm(passengers ~ seats * flights * month.chr * intra_regn + origin_state + dest_state + I(dist^2) * I(origin_pop^2) * I(dest_pop^2), data = flight_sub_train)

#Evaluate this model
train_rmse_best_guess = calc_rmse(resid(flight_model_best_guess))
test_rmse_best_guess  = calc_rmse(flight_sub_test$passengers -  
                                    predict(flight_model_best_guess, flight_sub_test) )

# List results
summary(flight_model_best_guess)$call

```

The regression of our initial best guess at a model proves to be significant with a p-value of `r pf(summary(flight_model_best_guess)$fstatistic[1], summary(flight_model_best_guess)$fstatistic[2], summary(flight_model_best_guess)$fstatistic[3], lower.tail = FALSE)`.  Both `seats` and `flights` were expected to be significant, as well as `dist^2`, as observed in the pairs plot.  Also significant was `intra_regn` and the vast majority of its two-way interactions.  `origin_pop` and `dest_pop` were not significant on their own; however, the interaction between `dist^2` and `origin_pop` **was** significant.  The interaction between `dist^2` and `dest_pop` was **not** significant.  This also might explain why BIC throws out the interaction with `dest_pop`.

*Model validation:*

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`

Although our educated best guess produced a model that fit the training data sample better, it did poorer on the test data sample. Running `step()` with AIC and BIC on this `fight_model_best_guess` did not produce a better model.

###A Simple Interactive Model

Our best model so far has been the `flight_model_big_bwd_aic` which started with all predictors and then used a backwards search using AIC to select $\beta$ parameters. We will start back with that model and add an interaction between the highly correlated `seats` and `flights` and recognize that `dist` has a quadratic relationship with `passengers`.

```{r cache = TRUE}
#All predictors plus and interaction between seats and flights and dist squared
flight_model_int = lm(passengers ~ . + seats * flights + I(dist ^ 2), data = flight_sub_train)

n = length(resid(flight_model_int))

#step selection
flight_model_int_bwd_aic = step(flight_model_int, direction = "backward", trace = 0)
flight_model_int_bwd_bic = step(flight_model_int, direction = "backward", k = log(n), trace = 0)

#Evaluate these models.

#We get a warning from predic(flight_model_int) about using a rank-deficient fit, so
#we will skip evaluating this model and use the step() selected models.

train_rmse_int_bwd_aic = calc_rmse(resid(flight_model_int_bwd_aic))

test_rmse_int_bwd_aic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_int_bwd_aic, flight_sub_test))

train_rmse_int_bwd_bic = calc_rmse(resid(flight_model_int_bwd_bic))

test_rmse_int_bwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_int_bwd_bic, flight_sub_test))

#List results
summary(flight_model_int_bwd_aic)$call
summary(flight_model_int_bwd_bic)$call

diagnostics(flight_model_int_bwd_aic, testit = FALSE)
```

This mode is statistically significant with a p-value of (`r pf(summary(flight_model_int_bwd_aic)$fstatistic[1], summary(flight_model_int_bwd_aic)$fstatistic[2], summary(flight_model_int_bwd_aic)$fstatistic[3], lower.tail = FALSE)`). The model does appear to violate assumptions of normality and homoscedasticity, but also makes a much better prediction than a response-transformed model.


*Model validation:*

model (passengers as response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`
flight_model_int_bwd_aic | `r summary(flight_model_int_bwd_aic)$adj.r.squared` | `r train_rmse_int_bwd_aic` | `r test_rmse_int_bwd_aic`
flight_model_int_bwd_bic | `r summary(flight_model_int_bwd_bic)$adj.r.squared` | `r train_rmse_int_bwd_bic` | `r test_rmse_int_bwd_bic`



### Forward Search

We also attempted to do a stepwise forward search which `R` usually picks the exact same model as in the backward stepwise search. Not shown, but a `"both"` stepwise search also arrives at the same model.

```{r cache = TRUE}
#Start with no predictors
flight_model_fwd = lm(passengers ~ 1, data = flight_sub_train)

n = nrow(flight_sub_train)

#step selection
flight_model_int_fwd_aic = step(flight_model_fwd, 
                                scope = passengers ~ origin_state + origin_regn + origin_subr + 
                                                     dest_state +   dest_regn +   dest_subr + 
                                                     intra_regn + intra_subr + seats * flights + 
                                                     dist + I(dist ^2) + year + month.chr +
                                                     origin_pop + dest_pop,
                                direction = "forward", trace = 0)

flight_model_int_fwd_bic = step(flight_model_fwd,
                                scope = passengers ~ origin_state + origin_regn + origin_subr + 
                                                     dest_state +   dest_regn +   dest_subr + 
                                                     intra_regn + intra_subr + seats * flights + 
                                                     dist + I(dist ^2) + year + month.chr +
                                                     origin_pop + dest_pop,
                                direction = "forward", k = log(n), trace = 0)

#Evaluate these models.

#We get a warning from predict(flight_model_int) about using a rank-deficient fit, so
#we will skip evaluating this model and use the step() selected models.

train_rmse_int_fwd_aic = calc_rmse(resid(flight_model_int_fwd_aic))

test_rmse_int_fwd_aic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_int_fwd_aic, flight_sub_test))

train_rmse_int_fwd_bic = calc_rmse(resid(flight_model_int_fwd_bic))

test_rmse_int_fwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   predict(flight_model_int_fwd_bic, flight_sub_test))

#List results
summary(flight_model_int_bwd_aic)$call
summary(flight_model_int_bwd_bic)$call

```

## Results

Here is summary of how well the models perform on the test dataset.

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`
**flight_model_int_bwd_aic** | `r summary(flight_model_int_bwd_aic)$adj.r.squared` | `r train_rmse_int_bwd_aic` | **`r test_rmse_int_bwd_aic`**
flight_model_int_bwd_bic | `r summary(flight_model_int_bwd_bic)$adj.r.squared` | `r train_rmse_int_bwd_bic` | `r test_rmse_int_bwd_bic`
flight_model_int_fwd_aic | `r summary(flight_model_int_fwd_aic)$adj.r.squared` | `r train_rmse_int_fwd_aic` | `r test_rmse_int_fwd_aic`
flight_model_int_fwd_bic | `r summary(flight_model_int_fwd_bic)$adj.r.squared` | `r train_rmse_int_fwd_bic` | `r test_rmse_int_fwd_bic`

Our chosen model is `flight_model_int_bwd_aic` based on having a good adjusted $R^2$ and consistently having the best root mean squared error (RMSE) on the test dataset. It's a complex model summarized here:

```{r}
summary(flight_model_int_bwd_aic)
```

Although our chosen model makes the best predictions, it does violate assumptions of normality and homoscedasticity.

```{r}
diagnostics(flight_model_int_bwd_aic, testit = FALSE)
```

## Discussion

This dataset representing all US domestic flights from 1990 to 2009 has over 3 million records, which is largely about moving `passengers`.  We explored the data in multiple ways with our primary focus on predicting `passengers`.  Although the data contains several numerical and categorical predictors, what has the largest influence on predicting `passengers` is the `seats` available and the number of `flights`.  This is not surprising as airlines seek to fill all `seats`.  Having pricing information would have made exploring this space much more interesting.

The small model that **explains** `passengers` in its easiest form is estimated from this sample as:

$$
passengers =  `r coef(flight_model_seats_flights)[1]` + `r coef(flight_model_seats_flights)[2]` * seats + `r coef(flight_model_seats_flights)[3]` * flights
$$
The regression shows that the more `seats` that are available between two points, the more `passengers` there are, however, the more `flights` there are between, reduces the `passengers`.  Presumably having more `flights` makes for a larger opportunity to have empty seats.  Using the adjusted $R^2$ value, this simple model explains about `r summary(flight_model_seats_flights)$adj.r.squared` of `passengers`. 

The model that predicts `passengers` the best is a complex model (`flight_model_int_bwd_aic`) that includes 51 levels for `origin_state`, 51 levels for `dest_state`, 20 levels for `year` and 12 levels for `month.chr`. Having all these beta parameters makes this much harder to explain, but it does consistently predict best on the test dataset. We tried several different training/test set samples and `flight_model_int_bwd_aic` consistently provided the best prediction as measured by the root mean square error (RMSE).  However, the gain in prediction accuracy doesn't appear to be very large as shown in the following table comparing the simple model to the complex model, due to `seats` and `flights` already explaining most of `passengers`.

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_int_bwd_aic | `r summary(flight_model_int_bwd_aic)$adj.r.squared` | `r train_rmse_int_bwd_aic` | `r test_rmse_int_bwd_aic`

Model `flight_model_int_bwd_aic` was arrived at with a backwards stepwise search starting with all predictors and included an interaction between `seats` and `flights` and a quadratic term for `dist` which is clearly seen in the pairs plot given above. AIC was the metric used to select predictors. The same model was also arrived at with a forward search using AIC as well as a forward/backward stepwise search. Searching with the BIC criteria did not yield models that tested as well.

What is interesting about the AIC backwards search is that it kept most of the numeric predictors but discarded the `origin_pop`.  The search did find `dest_pop` was significant. The search also discarded the regional predictors we added initially with the intention of reducing complexity of the model.  We never attempted to fit a model with the origin / destination cities.  What we learned while testing models is the training dataset must have all factor levels present in order to make predictions on unseen test data.  That would be a hard sample to make.  The AIC search also found that the flight date (month, year) was significant for making predictions.

Since the simple `flight_model_seats_flights` is nested in `flight_model_int_bwd_aic` we can use an ANOVA F-test to test for significance.

```{r}
(t1 = anova(flight_model_seats_flights, flight_model_int_bwd_aic))
```

`flight_model_int_bwd_aic` is statistically significant with a p-value of `r t1$"Pr(>F)"[2]`, making this the best model we could find to predict `passengers`.



>> How is your final model useful?

>> Discuss linear regression assumptions and what it means to choose a model that violates them

>> With collinearity, variance is much higher, but doesn't have a hugh effect on prediction


## Appendix

###Alternate Data Analysis Notes 

- `Passengers`, `Seats` and `Flights` are so highly correlated that using `Seats` or `Flights` as the response variable did not add anything to the data analysis.
- We were hoping to **observe** a better relationship between `passengers` and either `origin_pop` or `dest_pop`, but neither shows a distinguishable pattern in this sample.

###Stepwise "big" model diagnostics

When we initially started analyzing the big model selected by AIC & BIC, we ran into problems convincing ourselves that these models met normality and homoscedasticity assumptions.  Since our analysis focused upon prediction, these attempts were eventually abandoned, but are included for your amusement.

```{r fig.height=5, fig.width=10, echo = FALSE}
diagnostics(flight_model_best_guess, testit = FALSE)
```

While it is nice to have so many significant predictors, we appear to have an issue with both homoscedasticity and normality, confirmed by fitted vs. residuals and Q-Q plots.

We wished to perform a response variable transformation to address the clear pattern in the fitted vs. residuals plot.  To determine an appropriate transformation, we ran a Box-Cox log-likelihood plot:

```{r fig.width = 7, fig.height = 5}
boxcox(flight_model_best_guess, lambda = seq(0.55, 0.60, 1/100))
```

We chose a Box-Cox transformation with $\lambda = 0.575$.  

```{r}
lambda = 0.575

#Transform the response to improve on normality of the residual errors
transform = function(y) {
  ((y ^ lambda) - 1) / lambda
}

reverse_transform = function(y) {
  (y * lambda + 1) ^ (1 / lambda)
}

min_pass = min(flight_sub_train$passengers)
max_pass = max(flight_sub_train$passengers)

min_trans_pass = min(transform(flight_sub_train$passengers))
max_trans_pass = max(transform(flight_sub_train$passengers))

```

In the sampled dataset, the range of passengers is `r min_pass` to `r max_pass`. The Box-Cox transforms this into a much tighter range of `r min_trans_pass` to `r max_trans_pass`.

```{r}
#perform response transformation
flight_model_box = lm((passengers ^ lambda - 1) / lambda ~ 
                       seats * flights * month.chr * intra_regn + 
                       origin_state + dest_state + I(dist^2) * I(origin_pop^2) * I(dest_pop^2), 
                       data = flight_sub_train)
n = length(resid(flight_model_box))

#step selection
flight_model_box_bwd_aic = step(flight_model_box, direction = "backward", trace = 0)
flight_model_box_bwd_bic = step(flight_model_box, direction = "backward", k = log(n), trace = 0)

#Evaluate these models. Since the predictions are in the Box-Cox transformed units,
#we need to transform back for comparison with other models.

train_rmse_box = calc_rmse(flight_sub_train$passengers -
                           reverse_transform(predict(flight_model_box)))

test_rmse_box  = calc_rmse(flight_sub_test$passengers -
                           reverse_transform(predict(flight_model_box, flight_sub_test)))

train_rmse_box_bwd_aic = calc_rmse(flight_sub_train$passengers -
                                   reverse_transform(predict(flight_model_box_bwd_aic)))

test_rmse_box_bwd_aic  = calc_rmse(flight_sub_test$passengers -
                               reverse_transform(predict(flight_model_box_bwd_aic, flight_sub_test)))

train_rmse_box_bwd_bic = calc_rmse(flight_sub_train$passengers -
                                   reverse_transform(predict(flight_model_box_bwd_bic)))

test_rmse_box_bwd_bic  = calc_rmse(flight_sub_test$passengers -
                                   reverse_transform(predict(flight_model_box_bwd_bic, flight_sub_test)))

#List results
summary(flight_model_box_bwd_aic)$call
summary(flight_model_box_bwd_bic)$call

diagnostics(flight_model_box_bwd_aic, testit = FALSE)
```

While the Fitted vs. Residuals and Q-Q plots have improved dramatically, they are not quite ideal; however, they are probably as good as they are going to get, given the Box-Cox transformation we have chosen.

AIC still likes holding on to `origin_state` and `dest_state`, while BIC seems happy to remove them from the model. Both AIC & BIC retain some sort of region coding, however, as BIC keeps `intra_regn` in its model.


*Model validation:*

model (passengers response) | adj.r.squared | train RMSE | test RMSE
------|---------------|------------|------------|-----------
flight_model_seats_flights | `r summary(flight_model_seats_flights)$adj.r.squared` | `r train_rmse_seats_flights` | `r test_rmse_seats_flights`
flight_model_big_bwd_aic | `r summary(flight_model_big_bwd_aic)$adj.r.squared` | `r train_rmse_big_bwd_aic` | `r test_rmse_big_bwd_aic`
flight_model_big_bwd_bic | `r summary(flight_model_big_bwd_bic)$adj.r.squared` | `r train_rmse_big_bwd_bic` | `r test_rmse_big_bwd_bic`
flight_model_best_guess | `r summary(flight_model_best_guess)$adj.r.squared` | `r train_rmse_best_guess` | `r test_rmse_best_guess`
flight_model_box | `r summary(flight_model_box)$adj.r.squared` | `r train_rmse_box` | `r test_rmse_box`
flight_model_box_bwd_aic | `r summary(flight_model_box_bwd_aic)$adj.r.squared` | `r train_rmse_box_bwd_aic` | `r test_rmse_box_bwd_aic`
flight_model_box_bwd_bic | `r summary(flight_model_box_bwd_bic)$adj.r.squared` | `r train_rmse_box_bwd_bic` | `r test_rmse_box_bwd_bic`

However, as shown in the above model validation table, while this Box-Cox transformed model has improved normality and equal variance assumptions, it is has more errors in it's mean predictions and appears to have overfit the data. We tried an additional variety of predictors, with and without interactions, with and without quadratics, but could not find a model with a Box-Cox translated response that performed well on this sample.

###Looking at influential observations

During analysis when we were "tightening" up our model, we also took a look at influential observations, that usually fell into one of several categories:

* Short high volume flights
* Medium- to long-distance low volume flights

```{r, warning=FALSE}
n = length(resid(flight_model_box_bwd_aic))

percent = sum(cooks.distance(flight_model_box_bwd_aic) >
              (4 / length(cooks.distance(flight_model_box_bwd_aic))), na.rm = TRUE) / n * 100

plot(flight_model_box_bwd_aic, which = 6)
```

About `r percent`% of the training dataset are influential observations according to Cook's Distance heuristic.  Some of these observations are very large. However, these values appear to be valid data.  An example of a high influence observation comes from high volume short hops between the Hawaiian Islands. Removing some of the top influential observations does improved the residual errors slightly, but we cannot justify doing so.


